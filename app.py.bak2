import os
import re
import time
import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError

from flask import Flask, request, jsonify, render_template, redirect, url_for, flash, session
from flask_cors import CORS
from dotenv import load_dotenv
from flask_sqlalchemy import SQLAlchemy
from flask_login import (
    LoginManager, UserMixin, login_user, login_required, logout_user, current_user
)
from werkzeug.security import generate_password_hash, check_password_hash
from urllib.parse import urlparse, urljoin

import requests
from bs4 import BeautifulSoup

# --- Source list for multi_search ---
ALL_SOURCES = [
    "Reddit",
    "Stack Overflow",
    "ComplaintsBoard",
    "Product Hunt",
    "Quora"
]

# --- Load environment variables ---
load_dotenv()

# --- Gemini setup ---
import google.generativeai as genai
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
gemini_model = genai.GenerativeModel("gemini-2.0-flash")

# --- Reddit (PRAW) setup ---
import praw
reddit = praw.Reddit(
    client_id=os.getenv("REDDIT_CLIENT_ID"),
    client_secret=os.getenv("REDDIT_CLIENT_SECRET"),
    user_agent=os.getenv("REDDIT_USER_AGENT")
)

# --- Diskcache persistent cache ---
from diskcache import Cache
cache = Cache("radargpt_cache")

app = Flask(__name__)
CORS(app)

# --- Database and Login setup ---
app.config['SECRET_KEY'] = 'f3bce3d9a4b21e3d78d0f6a1c7eaf5a914ea7d1b2cfc9e4d8a237f6b5e8d4c13'
app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///users.db'
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False
db = SQLAlchemy(app)
login_manager = LoginManager()
login_manager.init_app(app)
login_manager.login_view = 'login'

# --- Models ---
class User(UserMixin, db.Model):
    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String(100), unique=True, nullable=False)
    password = db.Column(db.String(100), nullable=False)
    queries = db.relationship('SearchQuery', backref='user', lazy=True)

class SearchQuery(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    keyword = db.Column(db.String(200), nullable=False)
    source = db.Column(db.String(50), nullable=False)
    mode = db.Column(db.String(50), nullable=True)
    result = db.Column(db.Text, nullable=True)  # Stores summary
    timestamp = db.Column(db.DateTime, default=db.func.current_timestamp())
    user_id = db.Column(db.Integer, db.ForeignKey('user.id'), nullable=False)
    chats = db.relationship('QueryChat', backref='query', lazy=True)
    
class QueryChat(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    query_id = db.Column(db.Integer, db.ForeignKey('search_query.id'), nullable=False)
    role = db.Column(db.String(10), nullable=False)  # 'user' or 'bot'
    text = db.Column(db.Text, nullable=False)
    timestamp = db.Column(db.DateTime, default=datetime.datetime.utcnow)

@login_manager.user_loader
def load_user(user_id):
    return User.query.get(int(user_id))

def is_safe_url(target):
    ref_url = urlparse(request.host_url)
    test_url = urlparse(urljoin(request.host_url, target))
    return test_url.scheme in ('http', 'https') and ref_url.netloc == test_url.netloc

# --- Data Source Functions (with caching) ---
def get_reddit_posts_with_replies(keyword, start=0, batch_size=5, max_posts=5, subreddit="all"):
    cache_key = f"reddit_{keyword}_{start}_{batch_size}_{max_posts}_{subreddit}"
    result = cache.get(cache_key)
    if result is not None:
        return result
    results = []
    submissions = reddit.subreddit(subreddit).search(keyword, limit=None if start or batch_size else max_posts)
    for _ in range(start):
        try:
            next(submissions)
        except StopIteration:
            break
    for _ in range(batch_size if batch_size else max_posts):
        try:
            submission = next(submissions)
            submission.comments.replace_more(limit=0)
            comments = [c.body or "" for c in submission.comments.list()[:10]]
            results.append({
                "title": submission.title or "",
                "selftext": submission.selftext or "",
                "url": submission.url or "",
                "comments": comments
            })
        except StopIteration:
            break
    cache.set(cache_key, results, expire=3600)
    return results

def summarize_post_and_replies(post):
    prompt = f"""The following is a Reddit post and its top replies.
Extract and summarize problems/pain points and group them clearly.

POST:
Title: {post['title']}
Body: {post['selftext']}

COMMENTS:
{chr(10).join(post['comments'])}"""
    try:
        response = gemini_model.generate_content(prompt)
        return response.text.strip() if response.text else ""
    except Exception as e:
        return f"Error: {e}"

def generate_startup_ideas(summary):
    prompt = f"""Here is a problem summary:

{summary}

Generate top 3 startup ideas. For each:
- Problem
- Idea
- Existing Solutions
- Gaps
- Validation (1-10)"""
    try:
        response = gemini_model.generate_content(prompt)
        return response.text.strip() if response.text else ""
    except Exception as e:
        return f"Error: {e}"

def search_stackoverflow(keyword, max_pages=3, pagesize=50):
    cache_key = f"stackoverflow_{keyword}_{max_pages}_{pagesize}"
    result = cache.get(cache_key)
    if result is not None:
        return result
    all_results = []
    url = "https://api.stackexchange.com/2.3/search/advanced"
    for page in range(1, max_pages + 1):
        resp = requests.get(url, params={
            "order": "desc",
            "sort": "relevance",
            "q": keyword,
            "site": "stackoverflow",
            "page": page,
            "pagesize": pagesize
        })
        if resp.status_code != 200:
            break
        data = resp.json()
        items = data.get("items", [])
        if not items:
            break
        all_results.extend([{"title": i.get("title", ""), "link": i.get("link", "")} for i in items])
        if not data.get("has_more", False):
            break
    cache.set(cache_key, all_results, expire=3600)
    return all_results

def scrape_complaintsboard_full_text(keyword, max_pages=20):
    cache_key = f"complaintsboard_{keyword}_{max_pages}"
    result = cache.get(cache_key)
    if result is not None:
        return result
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service
    from webdriver_manager.chrome import ChromeDriverManager
    from selenium.webdriver.common.by import By
    from selenium.webdriver.common.keys import Keys
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException

    options = webdriver.ChromeOptions()
    options.add_argument("--headless")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    # Use webdriver_manager to handle driver installation
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    try:
        search_url = f"https://www.complaintsboard.com/?search={keyword.replace(' ', '+')}"
        driver.get(search_url)

        try:
            # Try multiple CSS selectors to find complaint items
            try:
                WebDriverWait(driver, 30).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "span.search__item-text"))
                )
                print(f"Found search__item-text elements for keyword: {keyword}")
            except TimeoutException:
                try:
                    WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, ".complaint-item"))
                    )
                    print(f"Found complaint-item elements for keyword: {keyword}")
                except TimeoutException:
                    print(f"No complaint elements found for keyword: {keyword}")
                    cache.set(cache_key, [], expire=3600)
                    return []
        except Exception as e:
            print(f"Error during ComplaintsBoard search: {e}")
            cache.set(cache_key, [], expire=3600)
            return []
            
        results = []
        page = 1
        seen = set()
        while page <= max_pages:
            for _ in range(5):
                driver.find_element(By.TAG_NAME, "body").send_keys(Keys.END)
                time.sleep(1)
            soup = BeautifulSoup(driver.page_source, "html.parser")
            
            # Try multiple selectors to find complaint items
            items = soup.select("span.search__item-text")
            if not items:
                items = soup.select(".complaint-item")
                
            for item in items:
                # For search__item-text elements
                parent_a = item.find_parent("a", href=True)
                if not parent_a:
                    # For complaint-item elements
                    parent_a = item.find("a", href=True)
                
                if parent_a:
                    href = parent_a['href']
                    # Fix: Use a more lenient regex pattern to match complaint URLs
                    # Some URLs might not have the #cXXX format
                    if href.startswith('/'):
                        page_url = "https://www.complaintsboard.com" + href.split('#')[0]
                        complaint_id = href.split('#')[1] if '#' in href else ''
                        key = (page_url, complaint_id)
                        if key not in seen:
                            results.append({
                                "title": item.text.strip(),
                                "url": f"{page_url}#{complaint_id}" if complaint_id else page_url,
                                "text": ""
                            })
                            seen.add(key)
            try:
                next_button = driver.find_element(By.XPATH, "//a[contains(., 'Next')]")
                if next_button.is_enabled():
                    next_button.click()
                    page += 1
                    time.sleep(2)
                else:
                    break
            except Exception:
                break
        cache.set(cache_key, results, expire=3600)
        return results
    finally:
        driver.quit()

def scrape_producthunt_products(keyword, max_pages=3, max_products=30):
    cache_key = f"producthunt_{keyword}_{max_pages}_{max_products}"
    result = cache.get(cache_key)
    if result is not None:
        return result
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.common.keys import Keys
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC

    options = webdriver.ChromeOptions()
    options.add_argument("--headless")
    options.add_argument("--window-size=1920,1080")
    driver = webdriver.Chrome(options=options)
    try:
        search_url = f"https://www.producthunt.com/search?q={keyword.replace(' ', '+')}"
        driver.get(search_url)

        products = []
        current_page = 1

        while current_page <= max_pages and len(products) < max_products:
            try:
                WebDriverWait(driver, 20).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "div.text-16.font-semibold.text-dark-gray"))
                )
            except Exception:
                break
            for _ in range(2):
                driver.find_element(By.TAG_NAME, "body").send_keys(Keys.END)
                time.sleep(1)
            soup = BeautifulSoup(driver.page_source, "html.parser")
            name_divs = soup.select('div.text-16.font-semibold.text-dark-gray')
            for name_div in name_divs:
                name = name_div.get_text(strip=True)
                desc_div = name_div.find_next_sibling('div', class_='text-14 font-normal text-light-gray')
                desc = desc_div.get_text(strip=True) if desc_div else ""
                parent_a = name_div.find_parent('a', href=True)
                url = "https://www.producthunt.com" + parent_a['href'] if parent_a else ""
                prod = {
                    "title": name,
                    "description": desc,
                    "url": url,
                    "text": desc
                }
                if prod not in products:
                    products.append(prod)
                if len(products) >= max_products:
                    break
            try:
                next_button = driver.find_element(By.XPATH, "//button[contains(., 'Next')]")
                if next_button.is_enabled():
                    next_button.click()
                    time.sleep(1)
                    current_page += 1
                else:
                    break
            except Exception:
                break
        cache.set(cache_key, products, expire=3600)
        return products
    finally:
        driver.quit()

def gemini_summarize_with_citations(content_list, prompt_prefix):
    sources_str = ""
    for idx, item in enumerate(content_list, 1):
        sources_str += f"[{idx}] {item['title']} ({item.get('url','')})\n"
    content_str = "\n\n".join([f"{item['title']}\n{item.get('text','')}" for item in content_list])
    prompt = (
        f"{prompt_prefix}\n"
        "After each fact or sentence, add a citation in the form [n] that refers to the n-th source in the list below. "
        "Only use citations for facts you can attribute to a specific source.\n\n"
        f"Sources:\n{sources_str}\n\nContent:\n{content_str}\n"
    )
    try:
        response = gemini_model.generate_content(prompt)
        return response.text.strip() if response.text else ""
    except Exception as e:
        return f"Error: {e}"

MODE_PROMPTS = {
    "future_pain": """
You're an expert product strategist and market analyst.
Based on long prompt understand the context or keyword : 
Step 1: List the most recent micro-trends in [industry] in detail based on the following user complaints and reviews from Reddit, Stack Overflow, Product Hunt, and ComplaintsBoard, in detail.
Step 2: Extract the most important emerging unsolved problems, with a focus on those likely to grow in the next 6â€“12 months based on the keyword and following user complaints and reviews from Reddit, Stack Overflow, Product Hunt, and ComplaintsBoard, in detail .
Include a trend heatmap with percentage change over time as bullet points.  
Step 3: Identify the main affected user personas or market segments for each problem.
Step 4: Explain products which are currently solving these problems, and their gaps from Product Hunt products and still what they are missing.
Step 4: Propose startup ideas or product features that directly address these pains.
Step 5: For each idea, score it on:
    - Novelty (1â€“10)
    - Urgency (1â€“10)
    - Feasibility (1â€“10)
    - Emotional intensity (anger/sadness/urgency in user language, 1â€“10)
    - Frequency (how often the problem appears across sources, 1â€“10)
    - Market size (potential number of users affected, 1â€“10)
generate a table with all these scores for each idea in good structure as seperate tables for each idea in structured format markdown table.
    
if keyword not found, return not found with this keyword.
- do not use tables, only use bullet points, but give a clear structure for each subheading and heading
- For each pain point, add a trend score (e.g., "ðŸ”¥ Trend score: +74% in last 6 months").
- If a table is too wide, split it into smaller tables.
- Give as seperate section blocks for each steps. 

Say what you do from all this u will likely to succeed in the next 6â€“12 months and why based on the trends and user needs.

After each fact or sentence, add a citation in the form [n] that refers to the n-th source in the list below. Only use citations for facts you can attribute to a specific source.

Content: {all_content} 
""",

    "unspoken": """
You are an expert in user psychology and product design.
Based on long prompt understand the context or keyword : 
Step 1: Carefully read the following multi-source user complaints, reviews, and replies.
Step 2: Extract hidden, emotional, or unspoken problems that users may not state directly, but which are implied by the language or context.
Step 3: Identify the psychological or behavioral patterns underlying these pains.
Step 4: Propose concrete product or UX fixes that would address these unspoken needs.
Step 5: For each problem, score:
    - Emotional intensity (anger/sadness/urgency, 1â€“10)
    - Frequency (how often it appears or is implied, 1â€“10)
    - Market size (potential users affected, 1â€“10)

- Include a trend heatmap or trend score column if possible.
- do not use tables, only use bullet points, but give a clear structure for each subheading and heading
- For each pain point, add a trend score (e.g., "ðŸ”¥ Trend score: +74% in last 6 months").
- If a table is too wide, split it into smaller tables.
- Give as seperate section blocks for each steps. 
After each fact or sentence, add a citation in the form [n] that refers to the n-th source in the list below. Only use citations for facts you can attribute to a specific source.

Content: {all_content}
""",

    "zero_to_one": """
You are a bold startup founder and innovation strategist.
Based on long prompt understand the context or keyword : 
Step 1: Review the following content for micro-trends and emerging unsolved problems.
Step 2: Identify affected user personas and market segments.
Step 3: Propose radical, zero-to-one startup ideas or new categories of products that could disrupt the market.
Step 4: For each idea, provide:
    - The specific problem it solves
    - The core idea/solution
    - Existing solutions and their gaps along with Product Hunt products and web explain what it does and what it missing.
    - Validation score (1â€“10)
    - Novelty, urgency, feasibility (1â€“10 each)
    - Emotional intensity, frequency, and market size (1â€“10 each)
    - If possible, a trend/heatmap score for the problem or idea
- do not use tables, only use bullet points, but give a clear structure for each subheading and heading
- For each pain point, add a trend score (e.g., "ðŸ”¥ Trend score: +74% in last 6 months").
- Add citations [n] for each fact or insight as above.

Content: {all_content}
"""
}


# --- 30-Second Timeout Multi-Source Search ---
def multi_source_search(keyword):
    results = {}
    start_time = time.time()
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = {
            "reddit": executor.submit(get_reddit_posts_with_replies, keyword),
            "stackoverflow": executor.submit(search_stackoverflow, keyword),
            "complaintsboard": executor.submit(scrape_complaintsboard_full_text, keyword),
            "producthunt": executor.submit(scrape_producthunt_products, keyword)
        }
        for name, future in futures.items():
            try:
                # Increase timeout for complaintsboard
                if name == "complaintsboard":
                    timeout = 60  # Give more time for ComplaintsBoard
                else:
                    timeout = max(5, 30 - (time.time() - start_time))
                    
                results[name] = future.result(timeout=timeout)
                print(f"Successfully retrieved {len(results[name])} results from {name}")
            except TimeoutError:
                print(f"Timeout error for {name}")
                results[name] = []
            except Exception as e:
                print(f"Error for {name}: {str(e)}")
                results[name] = []
    return results

# --- MAIN ROUTES ---

@app.route('/')
def home():
    return render_template('main.html')

@app.route('/radargpt', methods=['POST'])
@login_required
def radargpt_api():
    data = request.json
    keyword = data.get('keyword')
    mode = data.get('mode', 'future_pain')
    if not keyword:
        return jsonify({"error": "Keyword required"}), 400

    with ThreadPoolExecutor(max_workers=2) as executor:
        try:
            search_future = executor.submit(multi_source_search, keyword)
            search_results = search_future.result(timeout=15)
            all_content = []
            for v in search_results.values():
                if isinstance(v, list):
                    all_content.extend(v)
                elif isinstance(v, dict):
                    all_content.append(v)
            prompt_prefix = MODE_PROMPTS.get(mode, MODE_PROMPTS['future_pain'])
            summary_future = executor.submit(
                gemini_summarize_with_citations, all_content, prompt_prefix
            )
            summary = summary_future.result(timeout=15)
        except TimeoutError:
            return jsonify({"error": "Operation timed out"}), 504
        except Exception as e:
            return jsonify({"error": str(e)}), 500

    # Save to database
    new_query = SearchQuery(
        keyword=keyword,
        source="multi",
        mode=mode,
        result=summary,
        user_id=current_user.id
    )
    db.session.add(new_query)
    db.session.commit()

    return jsonify({
        "summary": summary,
        "sources": search_results,
        "query_id": new_query.id
    })

@app.route('/findradar', methods=['POST'])
@login_required
def findradar_api():
    data = request.json
    keyword = data.get('keyword')
    if not keyword:
        return jsonify({"error": "Keyword required"}), 400
    try:
        with ThreadPoolExecutor() as executor:
            future = executor.submit(multi_source_search, keyword)
            results = future.result(timeout=30)
    except TimeoutError:
        return jsonify({"error": "Search timed out"}), 504
    except Exception as e:
        return jsonify({"error": str(e)}), 500
    return jsonify({
        "results": results
    })

# --- User & Chat Management (unchanged) ---


@app.route('/app')
@login_required
def findradar():
    return render_template('index.html')

@app.route('/saved')
@login_required
def saved():
    return render_template('saved.html')

@app.route('/login', methods=['GET', 'POST'])
def login():
    if request.method == 'POST':
        username = request.form['username']
        password = request.form['password']
        user = User.query.filter_by(username=username).first()
        if user and check_password_hash(user.password, password):
            login_user(user)
            next_page = request.args.get('next')
            if not next_page or not is_safe_url(next_page):
                next_page = url_for('home')
            return redirect(next_page)
        else:
            flash('Invalid username or password')
    return render_template('login.html')

@app.route('/register', methods=['GET', 'POST'])
def register():
    if request.method == 'POST':
        username = request.form['username']
        password = generate_password_hash(request.form['password'])
        if User.query.filter_by(username=username).first():
            flash('User already exists')
            return redirect(url_for('register'))
        new_user = User(username=username, password=password)
        db.session.add(new_user)
        db.session.commit()
        flash('Registration successful! Please login.')
        return redirect(url_for('login'))
    return render_template('register.html')

@app.route('/logout')
@login_required
def logout():
    logout_user()
    flash('You have been logged out.')
    return redirect(url_for('home'))

@app.route('/chat/<int:query_id>', methods=['POST'])
@login_required
def chat(query_id):
    query = SearchQuery.query.get_or_404(query_id)
    if query.user_id != current_user.id:
        return jsonify({"error": "Unauthorized"}), 403

    data = request.get_json()
    user_text = data.get('text', '').strip()
    if not user_text:
        return jsonify({"error": "Empty input"}), 400

    user_msg = QueryChat(query_id=query_id, role='user', text=user_text)
    db.session.add(user_msg)

    base_context = {
        "role": "user",
        "parts": [f"""This is a follow-up question based on the startup radar summary below.

Original query keyword: {query.keyword}

Original result summary:
{query.result or "No summary available."}

Follow-up question: {user_text}"""]
    }
    chat_history = []
    for m in query.chats:
        role = "user" if m.role == "user" else "model"
        chat_history.append({"role": role, "parts": [m.text]})
    full_history = [base_context] + chat_history

    try:
        chat = gemini_model.start_chat(history=full_history)
        bot_response = chat.send_message(user_text).text.strip()
    except Exception as e:
        bot_response = f"Error: {e}"

    bot_msg = QueryChat(query_id=query_id, role='model', text=bot_response)
    db.session.add(bot_msg)
    db.session.commit()

    return jsonify({"bot_reply": bot_response})

@app.route('/radargpt')
@login_required
def radargpt():
    # Only send IDs and keywords to the template
    queries = SearchQuery.query.filter_by(user_id=current_user.id)\
        .order_by(SearchQuery.timestamp.desc()).all()
    return render_template('radargpt.html', queries=queries)

@app.route('/query_api/<int:query_id>')
@login_required
def query_api(query_id):
    query = SearchQuery.query.get_or_404(query_id)
    if query.user_id != current_user.id:
        return jsonify({"error": "Unauthorized"}), 403
    return jsonify({
        "id": query.id,
        "keyword": query.keyword,
        "summary": query.result,
        "timestamp": query.timestamp.isoformat()
    })

# --- Utility Endpoints (unchanged) ---
@app.route("/fetch_posts", methods=["POST"])
def fetch_posts():
    data = request.get_json()
    keyword = data.get("keyword", "")
    start = int(data.get("start", 0))
    batch_size = int(data.get("batch_size", 5))
    posts = get_reddit_posts_with_replies(keyword, start, batch_size)
    with ThreadPoolExecutor() as executor:
        summaries = list(executor.map(summarize_post_and_replies, posts))
        ideas = list(executor.map(generate_startup_ideas, summaries))
    for i, post in enumerate(posts):
        post["problems_summary"] = summaries[i]
        post["suggestions"] = ideas[i]
    has_more = len(posts) == batch_size
    return jsonify({"posts": posts, "has_more": has_more})

@app.route("/multi_search", methods=['POST'])
@login_required
def multi_search():
    try:
        data = request.get_json()
        keyword = data.get("keyword", "").strip()
        mode = data.get("mode", "future_pain")

        if not keyword:
            return jsonify({"error": "Missing keyword"}), 400

        sources = {src: [] for src in ALL_SOURCES}
        errors = {}

        def reddit_job():
            try:
                results = get_reddit_posts_with_replies(keyword, max_posts=5)
                sources["Reddit"] = [{"title": r["title"], "url": r["url"], "text": ""} for r in results]
            except Exception as e:
                errors["Reddit"] = str(e)
                sources["Reddit"] = []

        def stackoverflow_job():
            try:
                results = search_stackoverflow(keyword, max_pages=2, pagesize=25)
                sources["Stack Overflow"] = [{"title": s["title"], "url": s["link"], "text": ""} for s in results]
            except Exception as e:
                errors["Stack Overflow"] = str(e)
                sources["Stack Overflow"] = []

        def complaints_job():
            try:
                results = scrape_complaintsboard_full_text(keyword, max_pages=20)
                sources["ComplaintsBoard"] = [{"title": c["title"], "url": c["url"], "text": ""} for c in results]
            except Exception as e:
                errors["ComplaintsBoard"] = str(e)
                sources["ComplaintsBoard"] = []

        def producthunt_job():
            try:
                results = scrape_producthunt_products(keyword, max_pages=3, max_products=30)
                sources["Product Hunt"] = [{"title": p["title"], "url": p["url"], "text": p["description"]} for p in results]
            except Exception as e:
                errors["Product Hunt"] = str(e)
                sources["Product Hunt"] = []

        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = [
                executor.submit(reddit_job),
                executor.submit(stackoverflow_job),
                executor.submit(complaints_job),
                executor.submit(producthunt_job),
            ]
            for future in as_completed(futures):
                future.result()

        all_posts = []
        for src in ALL_SOURCES:
            all_posts.extend(sources[src])

        prompt_prefix = MODE_PROMPTS.get(mode, MODE_PROMPTS["future_pain"])
        summary = gemini_summarize_with_citations(all_posts, prompt_prefix)

        new_query = SearchQuery(
            keyword=keyword,
            source="multi",
            mode=mode,
            result=summary,
            user_id=current_user.id
        )
        db.session.add(new_query)
        db.session.commit()

        return jsonify({
            "summary": summary,
            "sources": sources,
            "query_id": new_query.id,
            "errors": errors if errors else None
        })

    except Exception as e:
        return jsonify({"error": f"Server error: {str(e)}"}), 500

def scrape_reddit_titles(keyword):
    results = []
    for submission in reddit.subreddit("all").search(keyword, limit=50):
        results.append({
            "title": submission.title or "",
            "link": f"https://reddit.com{submission.permalink}" if submission.permalink else "",
            "summary": (submission.selftext or "")[:300],
            "published": datetime.datetime.utcfromtimestamp(submission.created_utc).isoformat() if submission.created_utc else ""
        })
    return results

def search_stackexchange(keyword, max_pages=3, pagesize=50):
    all_results = []
    url = "https://api.stackexchange.com/2.3/search/advanced"
    for page in range(1, max_pages + 1):
        resp = requests.get(url, params={
            "order": "desc",
            "sort": "relevance",
            "q": keyword,
            "site": "stackoverflow",
            "page": page,
            "pagesize": pagesize
        })
        if resp.status_code != 200:
            break
        data = resp.json()
        items = data.get("items", [])
        if not items:
            break
        all_results.extend([{"title": i.get("title", ""), "link": i.get("link", "")} for i in items])
        if not data.get("has_more", False):
            break
    return all_results

@app.route("/search", methods=["GET"])
def search_get():
    keyword = request.args.get("q", "")
    reddit_titles = scrape_reddit_titles(keyword)
    stack = search_stackexchange(keyword, max_pages=3, pagesize=50)
    blogs = []
    all_text = "\n".join([p.get("title", "") for p in reddit_titles] + [s.get("title", "") for s in stack])

    summary_prompt = f"""
You are an AI research assistant.
Based on long prompt understand the context or keyword : 
Analyze the following multi-source discussions, posts, reviews, and articles related to the keyword. These are collected from various platforms such as Reddit, Twitter, Amazon reviews, Google search results, Hacker News, and blog posts.

Content to analyze:
{all_text}

Your task is to extract insights and structure the summary under the following clear sections:

<b>Pain Points:</b>
- Identify the most commonly mentioned or implied problems and frustrations.
- Include the source context where applicable (e.g., "From Reddit user", "Amazon review", "Tweet", "Google result").
- Even if issues are not directly related to the keyword, explain how they are connected.
- Mention who is affected by each issue (e.g., users, buyers, students, founders, etc.).

<b>Challenges:</b>
- Highlight the key difficulties or bottlenecks users face.
- Include both technical and emotional/behavioral challenges.
- Clarify how each challenge ties back to the keyword, even indirectly.
- Use real context or quotes from the source if it helps clarify the challenge.

<b>Insights:</b>
- Summarize startup opportunities, emerging trends, or unmet user needs based on the content.
- Highlight any ideas or recurring themes that indicate what users wish existed.
- Use insights that apply broadly or from niche user groups.
- Make sure insights are based on actual user pain rather than speculation.

Formatting:
- Use bullet points under each heading.
- Group similar insights or problems for clarity.
- Maintain factual tone and avoid assumptions not backed by source content.
"""
    try:
        response = gemini_model.generate_content(summary_prompt)
        return jsonify({
            "reddit": reddit_titles,
            "stack": stack,
            "blogs": blogs,
            "summary": response.text.strip() if response.text else ""
        })
    except Exception as e:
        return jsonify({
            "reddit": reddit_titles,
            "stack": stack,
            "blogs": blogs,
            "summary": f"Error generating summary: {e}"
        })

@app.route('/search', methods=['POST'])
@login_required
def search_post():
    keyword = request.form['keyword']
    source = request.form['source']
    new_query = SearchQuery(keyword=keyword, source=source, user_id=current_user.id)
    db.session.add(new_query)
    db.session.commit()
    return "Query saved!"

if __name__ == "__main__":
    with app.app_context():
        db.create_all()
    app.run(debug=True)
